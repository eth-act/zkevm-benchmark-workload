#!/usr/bin/env python3
"""
Convert zkevm-metrics-* directories to markdown format.

This script processes zkevm-metrics directories generated by the benchmark runner
and creates markdown reports showing proving metrics in a tabular format.
"""

import argparse
import json
import re
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import List, Optional

# Import the test name formatter
try:
    from test_name_formatter import TestNameFormatter
    FORMATTER_AVAILABLE = True
except ImportError:
    # If we can't import it, provide a fallback
    print("Warning: test_name_formatter module not found. Test names will not be formatted.", file=sys.stderr)
    FORMATTER_AVAILABLE = False


@dataclass
class MetricEntry:
    """Represents a single benchmark metric entry."""
    name: str
    gas_used: Optional[int]
    proof_size: Optional[int]
    proving_time_ms: Optional[float]
    peak_memory_bytes: Optional[int]
    avg_memory_bytes: Optional[int]
    initial_memory_bytes: Optional[int]
    version: str


@dataclass
class HardwareConfig:
    """Represents hardware configuration."""
    cpu_model: str
    total_ram_gib: int
    gpus: List[str]


def format_number(num: Optional[float], decimal_places: int = 1) -> str:
    """Format a number with thousands separators."""
    if num is None:
        return "N/A"
    if isinstance(num, int) or (isinstance(num, float) and num.is_integer()):
        return f"{int(num):,}"
    return f"{num:,.{decimal_places}f}"


def bytes_to_mb(bytes_val: Optional[int]) -> Optional[float]:
    """Convert bytes to megabytes."""
    if bytes_val is None:
        return None
    return bytes_val / (1024 * 1024)


def parse_hardware_json(hardware_file: Path) -> Optional[HardwareConfig]:
    """Parse the hardware.json file."""
    try:
        with open(hardware_file, 'r') as f:
            data = json.load(f)
            gpu_models = [gpu.get('model', 'Unknown GPU') for gpu in data.get('gpus', [])]
            return HardwareConfig(
                cpu_model=data.get('cpu_model', 'Unknown CPU'),
                total_ram_gib=data.get('total_ram_gib', 0),
                gpus=gpu_models
            )
    except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:
        print(f"Warning: Could not parse hardware.json: {e}")
        return None


def parse_metric_json(json_file: Path, version: str) -> Optional[MetricEntry]:
    """Parse a single metric JSON file."""
    try:
        with open(json_file, 'r') as f:
            data = json.load(f)
            
            name = data.get('name', json_file.stem)
            gas_used = data.get('metadata', {}).get('block_used_gas')
            
            # Extract proving metrics
            proving = data.get('proving', {})
            if 'success' in proving:
                success = proving['success']
                proof_size = success.get('proof_size')
                proving_time_ms = success.get('proving_time_ms')
                peak_memory = success.get('peak_memory_usage_bytes')
                avg_memory = success.get('average_memory_usage_bytes')
                initial_memory = success.get('initial_memory_usage_bytes')
            else:
                # If proving failed or data is missing
                proof_size = None
                proving_time_ms = None
                peak_memory = None
                avg_memory = None
                initial_memory = None
            
            return MetricEntry(
                name=name,
                gas_used=gas_used,
                proof_size=proof_size,
                proving_time_ms=proving_time_ms,
                peak_memory_bytes=peak_memory,
                avg_memory_bytes=avg_memory,
                initial_memory_bytes=initial_memory,
                version=version
            )
    except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:
        print(f"Warning: Could not parse {json_file}: {e}")
        return None


def collect_metrics_from_directory(metrics_dir: Path) -> tuple[Optional[HardwareConfig], List[MetricEntry]]:
    """Collect all metrics from a zkevm-metrics-* directory."""
    hardware_config = None
    metrics: List[MetricEntry] = []
    
    # Look for hardware.json in the root
    hardware_file = metrics_dir / 'hardware.json'
    if hardware_file.exists():
        hardware_config = parse_hardware_json(hardware_file)
    
    # Look for metrics in reth subdirectory structure
    reth_dir = metrics_dir / 'reth'
    if reth_dir.exists() and reth_dir.is_dir():
        # Iterate through version directories (e.g., sp1-v5.2.2, risc0-v3.0.3, etc.)
        for version_dir in reth_dir.iterdir():
            if version_dir.is_dir():
                version = version_dir.name
                # Process all JSON files in this version directory
                for json_file in sorted(version_dir.glob('*.json')):
                    metric = parse_metric_json(json_file, version)
                    if metric:
                        metrics.append(metric)
    
    # Also check for direct JSON files in the metrics directory (alternative structure)
    for json_file in sorted(metrics_dir.glob('*.json')):
        if json_file.name != 'hardware.json':
            # Extract version from parent directory name if possible
            version = metrics_dir.name.replace('zkevm-metrics-', '')
            metric = parse_metric_json(json_file, version)
            if metric:
                metrics.append(metric)
    
    return hardware_config, metrics


def generate_markdown(
    metrics_dir_name: str,
    hardware_config: Optional[HardwareConfig],
    metrics: List[MetricEntry],
    output_file: Path
) -> None:
    """Generate markdown report from metrics."""
    # Initialize the test name formatter if available
    formatter = TestNameFormatter() if FORMATTER_AVAILABLE else None
    
    # Extract zkVM version from the first metric (they should all be the same)
    zkvm_version = metrics[0].version if metrics and metrics[0].version else "Unknown"
    
    # Extract GPU count from directory name (e.g., "zkevm-metrics-sp1-1M-4" -> 4 GPUs)
    gpu_count_str = ""
    gpu_match = re.search(r'-(\d+)$', metrics_dir_name)
    if gpu_match:
        gpu_count = gpu_match.group(1)
        gpu_count_str = f" ({gpu_count} GPUs)"
    
    with open(output_file, 'w') as f:
        # Write header
        f.write("# zkEVM Benchmark Results\n\n")
        f.write(f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"## Folder: {metrics_dir_name}\n\n")
        
        # Write zkVM version with GPU count if available
        f.write(f"**zkVM:** {zkvm_version}{gpu_count_str}\n\n")
        
        # Write hardware configuration
        if hardware_config:
            gpu_list = ', '.join(hardware_config.gpus) if hardware_config.gpus else 'None'
            f.write(f"**Hardware Configuration:** CPU: {hardware_config.cpu_model} | ")
            f.write(f"RAM: {hardware_config.total_ram_gib} GiB | ")
            f.write(f"GPU: {gpu_list}\n\n")
        else:
            f.write("**Hardware Configuration:** Not available\n\n")
        
        # Write metrics table
        f.write("## Proving Metrics\n\n")
        
        if not metrics:
            f.write("No metrics data found.\n")
            return
        
        # Table header
        f.write("| Benchmark | Gas Used | Proof Size (bytes) | Proving Time (ms) | ")
        f.write("Proving Time (s) | Peak Memory (MB) | Avg Memory (MB) | Initial Memory (MB) |\n")
        f.write("|---|---|---|---|---|---|---|---|\n")
        
        # Table rows
        for metric in sorted(metrics, key=lambda m: m.name):
            # Format the test name using the formatter if available
            if formatter:
                try:
                    formatted_name = formatter.format_test_name(metric.name)
                except Exception:
                    # If formatting fails, use the original name
                    formatted_name = metric.name
            else:
                formatted_name = metric.name
            
            # Use the formatted name without version prefix
            benchmark_name = formatted_name
            
            gas_used_str = format_number(metric.gas_used)
            proof_size_str = format_number(metric.proof_size)
            proving_time_ms_str = format_number(metric.proving_time_ms)
            
            # Convert milliseconds to seconds
            proving_time_s = metric.proving_time_ms / 1000 if metric.proving_time_ms else None
            proving_time_s_str = format_number(proving_time_s, decimal_places=2)
            
            # Convert bytes to MB
            peak_memory_mb = bytes_to_mb(metric.peak_memory_bytes)
            avg_memory_mb = bytes_to_mb(metric.avg_memory_bytes)
            initial_memory_mb = bytes_to_mb(metric.initial_memory_bytes)
            
            peak_memory_str = format_number(peak_memory_mb)
            avg_memory_str = format_number(avg_memory_mb)
            initial_memory_str = format_number(initial_memory_mb)
            
            f.write(f"| {benchmark_name} | {gas_used_str} | {proof_size_str} | ")
            f.write(f"{proving_time_ms_str} | {proving_time_s_str} | ")
            f.write(f"{peak_memory_str} | {avg_memory_str} | {initial_memory_str} |\n")
        
        # Summary statistics
        f.write("\n## Summary Statistics\n\n")
        
        # Calculate statistics for successful tests
        successful_metrics = [m for m in metrics if m.proving_time_ms is not None]
        total_tests = len(metrics)
        successful_tests = len(successful_metrics)
        
        f.write(f"- **Total Tests:** {total_tests}\n")
        f.write(f"- **Successful Tests:** {successful_tests}\n")
        f.write(f"- **Failed Tests:** {total_tests - successful_tests}\n")
        
        if successful_metrics:
            # Proving time statistics
            proving_times = [m.proving_time_ms for m in successful_metrics if m.proving_time_ms is not None]
            if proving_times:
                avg_proving_time = sum(proving_times) / len(proving_times)
                min_proving_time = min(proving_times)
                max_proving_time = max(proving_times)
                f.write("\n### Proving Time (ms)\n")
                f.write(f"- **Average:** {format_number(avg_proving_time)}\n")
                f.write(f"- **Minimum:** {format_number(min_proving_time)}\n")
                f.write(f"- **Maximum:** {format_number(max_proving_time)}\n")
            
            # Memory statistics
            peak_memories = [m.peak_memory_bytes for m in successful_metrics if m.peak_memory_bytes is not None]
            if peak_memories:
                peak_memories_mb_raw = [bytes_to_mb(m) for m in peak_memories]
                peak_memories_mb = [mb for mb in peak_memories_mb_raw if mb is not None]
                if peak_memories_mb:
                    avg_peak_memory = sum(peak_memories_mb) / len(peak_memories_mb)
                    min_peak_memory = min(peak_memories_mb)
                    max_peak_memory = max(peak_memories_mb)
                    f.write("\n### Peak Memory Usage (MB)\n")
                    f.write(f"- **Average:** {format_number(avg_peak_memory)}\n")
                    f.write(f"- **Minimum:** {format_number(min_peak_memory)}\n")
                    f.write(f"- **Maximum:** {format_number(max_peak_memory)}\n")
            
            # Proof size statistics
            proof_sizes = [m.proof_size for m in successful_metrics if m.proof_size is not None]
            if proof_sizes:
                avg_proof_size = sum(proof_sizes) / len(proof_sizes)
                min_proof_size = min(proof_sizes)
                max_proof_size = max(proof_sizes)
                f.write("\n### Proof Size (bytes)\n")
                f.write(f"- **Average:** {format_number(avg_proof_size)}\n")
                f.write(f"- **Minimum:** {format_number(min_proof_size)}\n")
                f.write(f"- **Maximum:** {format_number(max_proof_size)}\n")


def process_single_directory(metrics_dir: Path, output_dir: Optional[Path] = None) -> None:
    """Process a single zkevm-metrics directory."""
    print(f"Processing: {metrics_dir}")
    
    hardware_config, metrics = collect_metrics_from_directory(metrics_dir)
    
    if not metrics:
        print(f"  Warning: No metrics found in {metrics_dir}")
        return
    
    print(f"  Found {len(metrics)} metric entries")
    
    # Determine output file path
    if output_dir:
        output_dir.mkdir(parents=True, exist_ok=True)
        output_file = output_dir / f"{metrics_dir.name}.md"
    else:
        output_file = metrics_dir.parent / f"{metrics_dir.name}.md"
    
    generate_markdown(metrics_dir.name, hardware_config, metrics, output_file)
    print(f"  Generated: {output_file}")


def main() -> int:
    """Main entry point for the script."""
    parser = argparse.ArgumentParser(
        description='Convert zkevm-metrics directories to markdown format',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Convert a single directory
  %(prog)s zkevm-metrics-sp1-1M-4

  # Convert multiple directories
  %(prog)s zkevm-metrics-sp1-1M-4 zkevm-metrics-risc0-1M

  # Convert all zkevm-metrics directories in current folder
  %(prog)s zkevm-metrics-*

  # Specify output directory
  %(prog)s -o zkevm-metrics-results zkevm-metrics-sp1-1M-4
        """
    )
    parser.add_argument(
        'directories',
        nargs='+',
        type=Path,
        help='One or more zkevm-metrics-* directories to convert'
    )
    parser.add_argument(
        '-o', '--output-dir',
        type=Path,
        help='Output directory for markdown files (default: same as input parent directory)'
    )
    
    args = parser.parse_args()
    
    # Process each directory
    processed_count = 0
    for metrics_dir in args.directories:
        if not metrics_dir.exists():
            print(f"Error: Directory {metrics_dir} does not exist")
            continue
        
        if not metrics_dir.is_dir():
            print(f"Error: {metrics_dir} is not a directory")
            continue
        
        process_single_directory(metrics_dir, args.output_dir)
        processed_count += 1
    
    print(f"\nProcessed {processed_count} directories")
    return 0 if processed_count > 0 else 1


if __name__ == '__main__':
    exit(main())

