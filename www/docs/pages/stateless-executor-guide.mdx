# Stateless Executor Benchmark Guide

This comprehensive guide explains how to run benchmarks using the **stateless-executor** guest program. The stateless-executor is the primary benchmarking workload that executes Ethereum stateless block execution logic without the pre and post execution validation overhead within zkVM environments.

---

## When to Use Stateless Executor

The `stateless-executor` is designed for specific benchmarking scenarios. Understanding when to use it helps ensure accurate measurements.

### Ideal Use Cases

| Use Case | Why Executor? |
|----------|---------------|
| **Opcode cost analysis** | Measures pure EVM execution cycles without validation noise |
| **EVM optimization research** | Isolates execution performance for comparison |
| **zkVM execution benchmarking** | Provides raw cycle counts for execution paths |
| **Gas cost modeling** | Accurate measurement of opcode-level costs |
| **Performance regression testing** | Consistent execution-only baseline for comparisons |

### When NOT to Use

| Scenario | Use Instead |
|----------|-------------|
| Production proving cost estimates | `stateless-validator` (includes full validation overhead) |
| Multi-client comparison (Reth vs Ethrex) | `stateless-validator` (supports both clients) |
| Realistic block validation benchmarks | `stateless-validator` (full validation pipeline) |
| zkVM baseline overhead measurement | `empty-program` (minimal workload) |

### Trade-offs to Consider

:::warning[Accuracy vs Isolation]
The executor provides **more accurate** measurement of pure EVM execution but **less realistic** overall proving costs. Choose based on your research goals:

- **Research goal: "How expensive is SLOAD in zkVM?"** → Use `stateless-executor`
- **Research goal: "How much to prove this block?"** → Use `stateless-validator`
:::

### Decision Criteria

1. **Are you measuring opcode costs?** → Use `stateless-executor`
2. **Do you need Ethrex support?** → Use `stateless-validator`
3. **Do you need realistic proving estimates?** → Use `stateless-validator`
4. **Are you comparing EVM implementations?** → Use `stateless-executor`

For a detailed comparison, see [Executor vs Validator](/executor-vs-validator).

---

## TLDR: Quick Start

For those who want to get started immediately with default settings:

```bash
# 1. Clone and navigate to the repository
git clone https://github.com/NethermindEth/zkevm-benchmark-workload.git
cd zkevm-benchmark-workload

# 2. Download EEST test fixtures
./scripts/download-and-extract-fixtures.sh benchmark@v0.0.4

# 3. Generate gas-categorized witness files
RAYON_NUM_THREADS=4 ./scripts/generate-gas-categorized-fixtures.sh

# 4. Run benchmarks (defaults: risc0 zkVM, reth execution client, GPU)
./scripts/run-gas-categorized-benchmarks.sh

```

**Default Configuration:**
- **zkVM**: RISC0
- **Execution Client**: Reth
- **Resource**: GPU
- **Gas Categories**: 1M, 10M, 30M, 45M, 60M, 100M, 150M
- **Action**: prove

:::warning[Execution Client Limitation]
The **stateless-executor** only supports the **Reth** execution client. Unlike `stateless-validator` which supports both Reth and Ethrex, the executor is limited to Reth. If you need Ethrex support, use [stateless-validator](/executor-vs-validator) instead.
:::

---

## Prerequisites

Before running benchmarks, ensure you have the following installed:

### Required Software

| Software | Minimum Version | Purpose |
|----------|-----------------|---------|
| **Docker** | 20.10+ | Required for EreDockerized zkVM compilation |
| **Rust** | 1.70+ | Building the benchmark runner |
| **Git** | 2.0+ | Cloning the repository |
| **Python** | 3.8+ | Running analysis scripts |
| **jq** | 1.6+ | JSON processing in shell scripts |
| **curl** | 7.0+ | Downloading fixtures |

### Hardware Requirements

| Resource | Minimum | Recommended |
|----------|---------|-------------|
| **CPU** | 8 cores | 16+ cores |
| **RAM** | 16 GB | 32+ GB |
| **GPU (optional)** | NVIDIA with CUDA | RTX 3080+ or A100 |
| **Disk Space** | 50 GB | 100+ GB |

### Verify Prerequisites

```bash
# Check Docker
docker --version
docker info  # Ensure Docker daemon is running

# Check Rust
rustc --version
cargo --version

# Check Python
python3 --version

# Check other tools
jq --version
curl --version
```

---

## Complete Workflow

The benchmarking process consists of four main stages:

```
┌──────────────────┐    ┌──────────────────┐    ┌──────────────────┐    ┌──────────────────┐
│  1. Download     │───▶│  2. Generate     │───▶│  3. Execute      │───▶│  4. Analyze      │
│     Fixtures     │    │     Witnesses    │    │     Benchmarks   │    │     Results      │
└──────────────────┘    └──────────────────┘    └──────────────────┘    └──────────────────┘
```

### Stage 1: Download EEST Fixtures

Download the Ethereum Execution Spec Test (EEST) fixtures that serve as the base test data.

```bash
# Download default fixtures (benchmark@v0.0.6)
./scripts/download-and-extract-fixtures.sh

# Download latest official release
./scripts/download-and-extract-fixtures.sh latest

# Download specific version
./scripts/download-and-extract-fixtures.sh v5.0.0

# Download to custom directory
./scripts/download-and-extract-fixtures.sh latest ./my-fixtures
```

**Output:** Creates `./zkevm-fixtures/` directory containing blockchain test fixtures.

### Stage 2: Generate Gas-Categorized Fixtures

Transform the EEST fixtures into `BlockAndWitness` JSON files organized by gas categories.

:::tip[Troubleshooting Parallelization]
If the process hangs or fails, try limiting parallelization by prefixing commands with `RAYON_NUM_THREADS=4`.
:::

```bash
# Generate with default gas categories (1M, 10M, 30M, 45M, 60M, 100M, 150M)
./scripts/generate-gas-categorized-fixtures.sh

# Generate with custom gas categories
./scripts/generate-gas-categorized-fixtures.sh -g 1M,10M,30M

# Use local EEST fixtures instead of downloading
./scripts/generate-gas-categorized-fixtures.sh -e ./my-local-eest-fixtures

# Preview commands without executing
./scripts/generate-gas-categorized-fixtures.sh --dry-run
```

**Output:** Creates directories like `./zkevm-fixtures-input-1M/`, `./zkevm-fixtures-input-10M/`, etc.

:::info[Fixture Compatibility]
The generated fixtures are compatible with both `stateless-executor` and `stateless-validator` guest programs. The fixture format (`StatelessValidationFixture`) contains:
- **Block data**: The Ethereum block to execute
- **Witness data**: State, bytecode, and ancestor headers needed for execution
- **Chain config**: Hardfork settings

See [Gas Categorized Fixtures](/gas-categorized-fixtures#fixture-format) for detailed fixture format documentation.
:::

### Stage 3: Execute Benchmarks

Run the stateless-executor benchmarks across your chosen configuration.

```bash
# Run all gas categories with defaults
./scripts/run-gas-categorized-benchmarks.sh

# Run with specific zkVM
./scripts/run-gas-categorized-benchmarks.sh -z sp1
./scripts/run-gas-categorized-benchmarks.sh -z risc0
./scripts/run-gas-categorized-benchmarks.sh -z openvm

# Run with specific execution client
./scripts/run-gas-categorized-benchmarks.sh -e ethrex

# Run on CPU instead of GPU
./scripts/run-gas-categorized-benchmarks.sh -r cpu

# Run only specific gas categories
./scripts/run-gas-categorized-benchmarks.sh -c 10M
./scripts/run-gas-categorized-benchmarks.sh -c 1M,10M,30M

# Enable memory tracking
./scripts/run-gas-categorized-benchmarks.sh -m

# Force rerun (bypass cached results)
./scripts/run-gas-categorized-benchmarks.sh -f

# Execute only (skip proving)
./scripts/run-gas-categorized-benchmarks.sh -a execute

# Custom input directory (default: ./zkevm-fixtures-input)
./scripts/run-gas-categorized-benchmarks.sh -i ./my-fixtures

# Custom output directory (default: ./zkevm-metrics)
./scripts/run-gas-categorized-benchmarks.sh -o ./my-metrics

# Custom input and output directories
./scripts/run-gas-categorized-benchmarks.sh -i ./my-fixtures -o ./my-metrics

# Preview commands without executing
./scripts/run-gas-categorized-benchmarks.sh -n
```

**Output:** Creates directories like `./zkevm-metrics-risc0-1M/`, `./zkevm-metrics-sp1-10M/`, etc.

### Stage 4: Analyze Results

Generate reports and compare benchmark results.

```bash
# Compare execution metrics between two runs
python3 scripts/compare_executions.py zkevm-metrics-risc0-1M zkevm-metrics-sp1-1M

# Compare proving times across all zkVMs (uses human-readable names by default)
python3 scripts/compare_proving_times.py

# Compare proving times with raw benchmark names (disable formatting)
python3 scripts/compare_proving_times.py --no-format

# Filter by zkVM or benchmark name
python3 scripts/compare_proving_times.py --zkvm sp1
python3 scripts/compare_proving_times.py --benchmark create

# Export proving times comparison to markdown
python3 scripts/compare_proving_times.py -o results/

# Convert metrics to markdown (uses human-readable names by default)
python3 scripts/convert-metrics-to-markdown.py zkevm-metrics-sp1-1M

# Convert multiple metrics directories
python3 scripts/convert-metrics-to-markdown.py zkevm-metrics-*

# Specify output directory for markdown files
python3 scripts/convert-metrics-to-markdown.py -o markdown-reports zkevm-metrics-sp1-1M
```

---

## Scripts Reference

### Setup Scripts

#### `download-and-extract-fixtures.sh`

Downloads and extracts Ethereum Execution Spec Test fixtures.

| Option | Description |
|--------|-------------|
| `[TAG]` | EEST release tag (e.g., `v5.0.0`, `latest`) |
| `[DEST_DIR]` | Destination directory (default: `./zkevm-fixtures`) |

**Environment Variables:**
- `GITHUB_TOKEN`: Optional. Provides authenticated API access to avoid rate limits.

```bash
# With authentication (recommended for CI/CD)
export GITHUB_TOKEN=ghp_xxxxxxxxxxxx
./scripts/download-and-extract-fixtures.sh latest
```

---

#### `generate-gas-categorized-fixtures.sh`

Generates witness files organized by gas categories.

| Option | Short | Description | Default |
|--------|-------|-------------|---------|
| `--gas` | `-g` | Comma-separated gas categories | 1M,10M,30M,45M,60M,100M,150M |
| `--eest-fixtures-path` | `-e` | Path to local EEST fixtures | - |
| `--dry-run` | - | Preview commands only | false |
| `--help` | `-h` | Show help message | - |

**Gas Category Format:** Use rational numbers with `M` suffix (e.g., `0.5M`, `1M`, `2.5M`, `10M`).

```bash
# Examples
./scripts/generate-gas-categorized-fixtures.sh -g 0.5M,1M,2.5M
./scripts/generate-gas-categorized-fixtures.sh -e ./eest-fixtures -g 10M,30M
```

---

### Benchmark Execution Scripts

#### `run-gas-categorized-benchmarks.sh`

Main script for running stateless-executor benchmarks.

| Option | Short | Description | Default |
|--------|-------|-------------|---------|
| `--dry-run` | `-n` | Preview commands only | false |
| `--force-rerun` | `-f` | Force rerun of benchmarks | false |
| `--action` | `-a` | Action: `prove` or `execute` | prove |
| `--resource` | `-r` | Resource: `gpu` or `cpu` | gpu |
| `--guest` | `-g` | Guest program type | stateless-executor |
| `--zkvm` | `-z` | zkVM implementation | risc0 |
| `--execution-client` | `-e` | Execution client | reth |
| `--input-dir` | `-i` | Base input directory | ./zkevm-fixtures-input |
| `--output-dir` | `-o` | Base output directory | ./zkevm-metrics |
| `--gas-categories` | `-c` | Gas categories to run | 1M,10M,30M,45M,60M,100M,150M |
| `--memory-tracking` | `-m` | Enable memory tracking | false |

**Supported zkVMs:**
- `risc0` - RISC0 zkVM (default)
- `sp1` - SP1 zkVM
- `openvm` - OpenVM zkVM
- `pico` - Pico zkVM
- `zisk` - Zisk zkVM
- `airbender` - Airbender zkVM

**Supported Execution Clients:**
- `reth` - Reth (default)
- `ethrex` - Ethrex

```bash
# Full example with all options
./scripts/run-gas-categorized-benchmarks.sh \
  -z sp1 \
  -e reth \
  -r gpu \
  -a prove \
  -c 1M,10M \
  -m \
  -f
```

---

### Analysis Scripts

#### `compare_executions.py`

Compares execution cycle counts between two benchmark runs.

```bash
python3 scripts/compare_executions.py <baseline_folder> <optimized_folder>

# Example
python3 scripts/compare_executions.py zkevm-metrics-risc0-1M zkevm-metrics-sp1-1M
```

**Output includes:**
- Speedup table by region (verify_witness, post_state_compute, validation, etc.)
- Total cycle counts comparison
- Statistical analysis with best/worst performers
- Key findings summary

---

#### `compare_provings.py`

Compares proving time metrics between two runs.

```bash
python3 scripts/compare_provings.py <baseline_folder> <optimized_folder>

# Example
python3 scripts/compare_provings.py zkevm-metrics-risc0-10M zkevm-metrics-sp1-10M
```

**Output includes:**
- Proving time speedup analysis
- Time savings calculations (in seconds)
- Efficiency gain reporting
- Top performers identification


---

#### `analyze_opcode_traces.py`

Analyzes bytecode and traces opcode execution.

```bash
python3 scripts/analyze_opcode_traces.py [OPTIONS]

# Options
--fixtures-dir <DIR>   Directory with test fixtures
--output <DIR>         Output directory for reports
--test-case <NAME>     Specific test case to analyze

# Example
python3 scripts/analyze_opcode_traces.py --fixtures-dir ./zkevm-fixtures --output ./opcode-analysis
```

---

## zkVM Compatibility

The stateless-executor supports multiple zkVM platforms. Each platform has been tested for compatibility with the executor guest program.

### Supported zkVM Platforms

| zkVM | Status | GPU Support | Notes |
|------|:------:|:-----------:|-------|
| **RISC0** | ✅ Supported | ✅ Yes | Default zkVM, well-tested, production-ready |
| **SP1** | ✅ Supported | ✅ Yes | Excellent GPU acceleration, fast proving |
| **OpenVM** | ✅ Supported | ✅ Yes | Good performance characteristics |
| **Pico** | ✅ Supported | ⚠️ Limited | Experimental GPU support |
| **Zisk** | ✅ Supported | ✅ Yes | Strong GPU performance |
| **Airbender** | ✅ Supported | ✅ Yes | Optimized for specific workloads |

### Platform Selection Guidelines

```bash
# RISC0 - Default choice, best documentation and support
./scripts/run-gas-categorized-benchmarks.sh -z risc0

# SP1 - Fast proving, excellent for large workloads
./scripts/run-gas-categorized-benchmarks.sh -z sp1

# OpenVM - Alternative with good performance
./scripts/run-gas-categorized-benchmarks.sh -z openvm

# Zisk - Strong GPU acceleration
./scripts/run-gas-categorized-benchmarks.sh -z zisk
```

### Performance Characteristics

Different zkVMs have different performance profiles:

| zkVM | Cycle Counting | Proving Speed | Memory Usage |
|------|----------------|---------------|--------------|
| **RISC0** | Accurate | Moderate | Moderate |
| **SP1** | Accurate | Fast | Higher |
| **OpenVM** | Accurate | Fast | Moderate |
| **Pico** | Accurate | Moderate | Lower |
| **Zisk** | Accurate | Fast | Higher |
| **Airbender** | Accurate | Variable | Variable |

:::info[Cycle Count Consistency]
All supported zkVMs provide consistent cycle counts for the same execution workload. The primary differences are in proving speed and memory requirements, not in the cycle measurement accuracy.
:::

### Running Multi-zkVM Comparisons

To compare the same workload across multiple zkVMs:

```bash
# Run the same gas category across different zkVMs
for zkvm in risc0 sp1 openvm zisk; do
  ./scripts/run-gas-categorized-benchmarks.sh \
    -z "$zkvm" \
    -c 1M \
    -o "./zkevm-metrics-${zkvm}"
done

# Compare results
python3 scripts/compare_proving_times.py
```

---

## Troubleshooting

### Stage 1: Download Fixtures

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **GitHub rate limit** | `API rate limit exceeded` error | Set `GITHUB_TOKEN` environment variable |
| **Network timeout** | Download stalls or fails | Retry with `curl --retry 3` or use VPN |
| **Asset not found** | `Asset not found in release` error | Check the release tag exists on GitHub |
| **Disk space** | Extraction fails | Ensure 10+ GB free space |

```bash
# Fix rate limit
export GITHUB_TOKEN=ghp_your_token_here
./scripts/download-and-extract-fixtures.sh

# Check disk space
df -h .
```

---

### Stage 2: Generate Fixtures

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Missing EEST fixtures** | `Fixtures directory not found` | Run download script first |
| **Invalid gas format** | `Invalid gas value format` error | Use format like `1M`, `0.5M`, `10M` |
| **Build failure** | Cargo compilation errors | Check Rust version, run `rustup update` |
| **Memory exhaustion** | Process killed | Reduce parallel jobs or increase RAM |

```bash
# Verify fixtures exist
ls -la ./zkevm-fixtures/

# Check Rust installation
rustup show
rustup update

# Build with verbose output
cargo build --release --bin witness-generator-cli -v
```

---

### Stage 3: Execute Benchmarks

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **Docker not running** | `Cannot connect to Docker daemon` | Start Docker: `sudo systemctl start docker` |
| **GPU not detected** | CUDA errors or fallback to CPU | Check `nvidia-smi`, use `-r cpu` flag |
| **Input not found** | `Input directory not found` | Run fixture generation first |
| **Out of memory** | Process killed, OOM errors | Use smaller gas categories or more RAM |
| **Build failure** | EreDockerized compilation errors | Check Docker disk space, restart Docker |

```bash
# Check Docker
docker info
docker ps

# Check GPU
nvidia-smi

# Check input directories
ls -la zkevm-fixtures-input-*

# Run with CPU if GPU issues
./scripts/run-gas-categorized-benchmarks.sh -r cpu

# Run smaller gas category first
./scripts/run-gas-categorized-benchmarks.sh -c 1M
```

---

### Stage 4: Analysis

| Problem | Symptoms | Solution |
|---------|----------|----------|
| **No common files** | `No common files found` | Ensure both folders have matching test names |
| **Import error** | `ModuleNotFoundError` | Run from scripts directory or fix PYTHONPATH |
| **Empty results** | No data in output | Check JSON files exist in metrics folders |

```bash
# Check metrics files exist
ls -la zkevm-metrics-*/

# Run from correct directory
cd /path/to/zkevm-benchmark-workload
python3 scripts/compare_executions.py ...

# Verify JSON structure
cat zkevm-metrics-risc0-1M/some-test.json | jq .
```

---

## Automation & Best Practices

### Environment Configuration

Create a `.env` file for consistent configuration:

```bash
# .env
export GITHUB_TOKEN=ghp_xxxxxxxxxxxx
export ZKVM=risc0
export EXECUTION_CLIENT=reth
export GAS_CATEGORIES=1M,10M,30M
export RESOURCE=gpu
```

Load before running:

```bash
source .env
./scripts/run-gas-categorized-benchmarks.sh -z $ZKVM -e $EXECUTION_CLIENT -c $GAS_CATEGORIES -r $RESOURCE
```

---

### Batch Processing Multiple zkVMs

```bash
#!/bin/bash
# run-all-zkvms.sh

ZKVMS=("risc0" "sp1")
GAS_CATEGORIES="1M,10M"

for zkvm in "${ZKVMS[@]}"; do
    echo "Running benchmarks for $zkvm..."
    ./scripts/run-gas-categorized-benchmarks.sh \
        -z "$zkvm" \
        -c "$GAS_CATEGORIES" \
        -f

    if [ $? -ne 0 ]; then
        echo "Failed: $zkvm"
        exit 1
    fi
done

echo "All benchmarks completed!"

# Generate comparison reports
for gas in 1M 10M; do
    echo "Comparing results for ${gas}..."
    python3 scripts/compare_executions.py \
        "zkevm-metrics-risc0-${gas}" \
        "zkevm-metrics-sp1-${gas}" \
        > "comparison-risc0-vs-sp1-${gas}.txt"
done
```

---

### Memory Tracking Best Practices

When using memory tracking (`-m` flag):

1. **Start with smaller gas categories** to establish baseline memory usage
2. **Monitor system memory** during execution: `watch -n 1 free -h`
3. **Allocate 2x expected memory** as headroom for peak usage
4. **Review memory metrics** in output JSON for optimization opportunities

```bash
# Run with memory tracking on small category first
./scripts/run-gas-categorized-benchmarks.sh -c 1M -m

# Check memory metrics
jq '.proving.success.peak_memory_usage_bytes' zkevm-metrics-risc0-1M/*.json
```

---

### Performance Optimization Tips

1. **Use GPU when available** - Proving is 5-10x faster with GPU
2. **Start small** - Test with 1M gas category before running larger ones
3. **Use dry-run first** - Preview commands with `-n` flag
4. **Parallelize zkVM runs** - Run different zkVMs on different machines
5. **Monitor disk space** - Metrics can consume significant space for large runs
6. **Cache fixtures** - Reuse generated fixtures across multiple benchmark runs

---

### Execution Time Estimates

| Gas Category | GPU (prove) | CPU (prove) | GPU (execute) | CPU (execute) |
|--------------|-------------|-------------|---------------|---------------|
| 1M | 5-15 min | 30-60 min | 1-2 min | 5-10 min |
| 10M | 15-30 min | 1-2 hr | 5-10 min | 20-40 min |
| 30M | 30-60 min | 2-4 hr | 10-20 min | 40-80 min |
| 45M | 45-90 min | 3-6 hr | 15-30 min | 1-2 hr |
| 60M | 60-120 min | 4-8 hr | 20-40 min | 1.5-3 hr |
| 100M | 90-180 min | 6-12 hr | 30-60 min | 2-4 hr |
| 150M | 120-240 min | 8-16 hr | 45-90 min | 3-6 hr |

*Times are estimates and vary based on hardware and zkVM implementation.*

---

## Output Structure

After running benchmarks, your directory structure will look like:

```
zkevm-benchmark-workload/
├── zkevm-fixtures/                    # Downloaded EEST fixtures
│   └── fixtures/
│       └── blockchain_tests/
│           └── benchmark/
├── zkevm-fixtures-input-1M/           # Generated witnesses (1M gas)
│   ├── test-case-1.json
│   ├── test-case-2.json
│   └── ...
├── zkevm-fixtures-input-10M/          # Generated witnesses (10M gas)
├── zkevm-metrics-risc0-1M/            # RISC0 results (1M gas)
│   ├── test-case-1.json               # Contains execution & proving metrics
│   └── ...
├── zkevm-metrics-sp1-1M/              # SP1 results (1M gas)
├── index.html                         # Generated HTML report
└── comparison-*.txt                   # Comparison reports
```

### Metrics JSON Structure

Each metrics file contains:

```json
{
  "name": "test-case-name",
  "execution": {
    "success": {
      "total_num_cycles": 5000000,
      "region_cycles": {
        "execution": 5000000
      },
      "execution_duration": {
        "secs": 45,
        "nanos": 123456789
      }
    }
  },
  "proving": {
    "success": {
      "proving_time_ms": 180000,
      "proof_size": 1024,
      "peak_memory_usage_bytes": 8589934592
    }
  }
}
```

:::info[Executor-Specific Metrics]
The `stateless-executor` only measures the **"execution"** region, which represents pure EVM transaction execution cycles. Unlike `stateless-validator`, the executor does not include `verify_witness`, `post_state_compute`, or `validation` regions because it skips all validation steps.

This makes the metrics directly reflect the cost of EVM opcode execution without validation overhead.
:::

---

For more detailed information on specific topics:

- [Gas Categorized Fixtures](/gas-categorized-fixtures) - Fixture generation details
- [Gas Categorized Benchmarks](/gas-categorized-benchmarks) - Benchmark execution details
- [Compare SP1 vs RISC0](/compare-sp1-risc0) - zkVM comparison workflow
- [Scripts Reference](/scripts) - Complete scripts documentation

